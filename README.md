# Data Lake

- [Data Lake](#data-lake)
  - [Summary](#summary)
  - [Datasets](#datasets)
    - [Song Dataset](#song-dataset)
      - [Log Dataset](#log-dataset)
  - [Quick Start](#quick-start)
    - [Prerequisites](#prerequisites)
    - [Option 1: Run spark cluster locally](#option-1-run-spark-cluster-locally)
    - [Option 2: Run spark cluster on AWS](#option-2-run-spark-cluster-on-aws)
      - [Start Up your EMR Cluster:](#start-up-your-emr-cluster)
      - [Connect to Master Node from a BASH shell and update the spark-env.sh file:](#connect-to-master-node-from-a-bash-shell-and-update-the-spark-envsh-file)
      - [Create a local tunnel to the EMR Spark History Server on your Linux machine:](#create-a-local-tunnel-to-the-emr-spark-history-server-on-your-linux-machine)
      - [Run your Spark job](#run-your-spark-job)
  - [Architecture](#architecture)
    - [Spark](#spark)
    - [AWS S3](#aws-s3)
    - [AWS EMR](#aws-emr)
  - [Structure](#structure)

## Summary
The purpose of this repo is to build a data lake using AWS S3 and spark to enable data modelling for Sparkify and their collection of songs and user activity from their music streaming app. This database will enable the analytics team to analyse the data and understand what songs users are listening to.

Data is loaded from an S3 bucket, processed into analytics tables with Spark and loaded back into S3. The pipeline is deployed using a spark cluster on AWS.

## Datasets

<details>
    <summary> Show/Hide Details</summary>

Their are two datasets that reside in S3. Here are the S3 links for each:

- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

- `song_data/A/B/C/TRABCEI128F424C983.json`
- `song_data/A/A/B/TRAABJL12903CDCF1A.json`

#### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from Sparkify's music streaming app.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

- `log_data/2018/11/2018-11-12-events.json`
- `log_data/2018/11/2018-11-13-events.json`

</details>
    
## Quick Start

### Prerequisites

<details>
    <summary> Show/Hide Details</summary>

1. Set up your AWS access key and secret access key within the [dl.cfg.example](dl.cfg.example) file and rename the file as `dl.cfg`:

```config
[AWS]
AWS_ACCESS_KEY_ID=<YOUR AWS ACCESS KEY>
AWS_SECRET_ACCESS_KEY=<YOUR AWS SECRET ACCESS KEY>

[S3]
INPUT_DATA=<YOUR INPUT DATA FOLDER / BUCKET>
OUTPUT_DATA=<YOUR OUTPUT DATA FOLDER / BUCKET>
```

2. Install AWS CLI locally: link [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)

3. Configure your AWS profile locally: `aws configure --profile <YOUR PROFILE NAME>`

</details>
    
### Option 1: Run spark cluster locally

<details open>
    <summary> Show/Hide Details</summary>

Within an environment configured to run spark, run the following command from your terminal:

```bash
python etl.py
```

</details>

### Option 2: Run spark cluster on AWS

<details>
    <summary> Show/Hide Details</summary>


#### Start Up your EMR Cluster:

Create an EMR Cluser via AWS CLI:

```bash
aws emr create-cluster --name my_spark_cluster \ 
--use-default-roles \
--release-label emr-5.28.0 \
--instance-count <INSTANCES INCLUDING MASTER NODE e.g 3> \
--applications Name=Spark \
--ec2-attributes KeyName=<YOUR PEM KEY NAME>,SubnetId=<YOUR SUBNET ID> \
--instance-type <EC2 instance type e.g m5.xlarge>
--profile <YOUR AWS PROFILE e.g default>
```

- Note your pem key name should exclude the `.pem` extension

#### Connect to Master Node from a BASH shell and update the spark-env.sh file:

1. Log onto the AWS Console and view the security group id for the Master Node via EC2 dashboard → Security Groups service. Edit the security group to authorize inbound SSH traffic (port 22) from your local computer.

2. Connect to the EMR cluster using the SSH protocol. Obtain your EC2 IP address for the master node from the AWS Console via EC2 dashboard → Instances.

```bash
ssh -i <PATH_TO_MY_KEY_PAIR_FILE>.pem hadoop@<EC2_IP_ADDRESS><YOUR_AWS_REGION>.compute.amazonaws.com
```

3. Using sudo, append the following line to the /etc/spark/conf/spark-env.sh file:

```bash
export PYSPARK_PYTHON=/usr/bin/python3
```

#### Create a local tunnel to the EMR Spark History Server on your Linux machine:

Open up a new Bash shell and run the following command (using the proper IP for your master node):

```bash
ssh -i <PATH_TO_MY_KEY_PAIR_FILE>.pem -N -L 8157:<EC2_IP_ADDRESS>.<YOUR_AWS_REGION>.compute.amazonaws.com:18080 hadoop@<EC2_IP_ADDRESS>.<YOUR_AWS_REGION>.compute.amazonaws.com
```

- Note: This establishes a tunnel between your local port 8157 and port 18080 on the master node.

You can pick a different unused number for your local port instead.

The list of ports on the EMR side and what UIs they offer can be found [here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)

3. Go to localhost:8157 in a web browser on your local machine and you should see the Spark History Server UI

#### Run your Spark job

1. SFTP the dl.cfg and etl.py files to the hadoop account directory on EMR.

2. In your home directory on the EMR master node (/home/hadoop), run the following command:

```bash
spark-submit --master yarn etl.py
```

- Note: full path to `etl.py` is required

3. After a couple of minutes your job should show up in the Spark History Server page in your browser

- You should see the real-time logging output in your EMR bash shell window as well

</details>


## Architecture

The components chosen for this framework should be able to accommodate most
ETL processes that require high scalability, assessibility and need to accomodate either structured or unstructured data types.

<details open>
    <summary> Show/Hide Details</summary>

### [Spark](https://spark.apache.org/)

Utilises spark via [pyspark](https://spark.apache.org/docs/latest/api/python/) for Spark SQL and dataframe functionality. This enables schema on read and acts as a distributed SQL query engine. As a result, the ETL job is significantly more efficient as a result of leveraging spark's distributed engine.


### [AWS S3](https://aws.amazon.com/products/storage/data-lake-storage/)

AWS S3 is used as the data lake as it provides high durability and ability to use native AWS services such as AWS EMR, AWS Glue, AWS lambda and AWS Athena. AWS S3 also supports unstructured and structured data types.

### [AWS EMR](https://aws.amazon.com/emr/)

Elastic Map Reduce (EMR) is a service offered by AWS that removes the need to install Spark and its dependencies. In addition, EMR handles node categorisation by categorising secondary nodes into core and task nodes. 

</details>

## Structure

<details open>
    <summary> Show/Hide Details</summary>

* [etl.py](etl.py): spark job to load data from s3, process data with spark and load back into S3
* [dl.cfg.example](dl.cfg.example): example config for your AWS credentials
* dl.cfg (included in gitignore)
* spark_job.log (included in gitignore): log file created for debugging purposes

</details>